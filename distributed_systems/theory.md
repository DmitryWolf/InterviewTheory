**Theory of fault-tolerant distributed systems**
# Лекция 1. Модель распределенной системы
## Зачем нужна распределенность
Современный мир без распределенных систем сложно себе представить

Виды:
* KV Store - гигантская хеш таблица (упорядоченная)
    * Set(k, v)
    * Get(k)
* File system
* Coordination Service - распределенные системы, которые нужны, чтобы строить другие системы
    * Например, ~Atomics
* Message Queues
* Databases

Зачем нам делать что-то распределенным?
1) В одну машину все не помещается
2) Распределенные системы - это не только про большие данные. Одна машина может отказать, система должна быть отказоустойчива
3) Мы не хотим доверять каким-то машинам, одной или несколько, они могут быть злоумышленниками

Для того, чтобы строить алгоритмы, мы должны иметь модель мира, в котором мы собираемся работать

## Общая модель: узлы, каналы, отправка сообщений, клиенты, истории, модель согласованности
Что такое распределенная система?
В отличие от алгоритмов, которые что-то сортируют или ищут, мы от модели ждем каких-то гарантий
В коде клиента система выглядит как переменная, и мы выполняем над ней какие-то операции. Мы не единственные, кто так делаем, нас таких много. Внутри системы удобно говорить про **модель MessagePassing**, а снаружи разумно использовать **модель SharedMemory**

**History**
![alt text](images/1.png)

## Моделирование сети
### Гарантии доставки
Что может с сообщением случиться? У нас есть абстракция надежного канала, мы в него отправляем сообщение из узла *a* в узел *b*, и узел *b* даже начинает что-то получать и сообщение обрабатывать, и вдруг соединение рвется (например, кто-то задел провод), и все гарантии в связи с этим разрывом пропадают
Соединения либо гарантируют нам доставку, либо сообщат, что оно порвалось
![alt text](images/2.png)
### Асинхронность и частичная синхронность
**Synchronous model**:
Есть константа $\delta$
$ delay(m) \le $ $\delta$ - задержка при доставки сообщения ограничена дельтой
Хорошая сеть может работать очень быстро, доставляя сообщение между двумя узлами за миллисекунду, почему бы не взять за дельту пару миллисекунд?
Такое бывает, но не всегда. Мы можем ожидать, что сеть работает быстро в каких-то частях нашего алгоритма, но мы хотим, чтобы наш алгоритм был устойчив к тому, что эта гарантия возьмет и нарушится, потому что мы физический мир не можем заставить так работать. Может быть куча случаев ошибки извне.

**Asynchronous model**:
Если наш алгоритм будет работать в таких предположениях, не ожидая верхней границы, то он сможет работать в реальном мире
* **Safety** свойство (минимальная задача алгоритма): система не должна уходить в плохие состояния, т.е. система не должна делать что-то плохое
* **Liveness** свойство: система должна делать что-то хорошее

**Partial synchrony**
У нас жизнь устроена так: есть ось времени, и некоторое время мы живем в асинхронной модели, тогда все плохо. А потом в какой-то волшебный момент __t*__ у нас эта гарантия начинает соблюдаться
![alt text](images/3.png)
## Партишены и split brain
**Partition** - это явление, когда сеть раскалывает кластер на две части, так, что связность в пределах одной части сохраняется, но линки, которые пересекают эти партишены, перестают работать. В итоге наша система раскалывается на две части, и хуже того, она еще и клиентов раскалывает
**Split brain** - ситуация, когда система в случае партишена начинает работать независимо в двух частях
![alt text](images/4.png)
## Моделирование узлов, сбои узлов
Каждый узел - это автомат или актор, который работает однопоточно и принимает сообщение из сети, и вызывает какой-то обработчик, и когда этот обработчик вызван, узел меняет свое внутреннее состояние, принимает какое-то новое, и, возможно, отвечает какими-то новыми сообщениями

Можно представить себе, что реальная программа исполняется на пуле потоков
Можно представить себе, что вот эти все файберы и фьючеры бегут на экзекуторе, которой по пинку выполняет очередную задачу

* Crash
* Restart
* Византийские отказы

## Время, часы и их применения
Есть какая-то ось времени, но доступа ко времени мы не имеем. У нас есть часы. В теории, часы - это такая функция, которая по времени возвращает нам некоторое значение (в идеале возвращает t, тождественная функция)
Часы - это объект физического мира. И, как и все в физическом мире, они несовершенны
А как мы собираемся использовать время в алгоритме?

У нас есть now(), есть t1 и t2, мы можем сделать t1 < t2 (сравнивать показания времени) и t2 - t1 (измерять интервал времени).
Зачем нам нужно измерять интервал времени? Для timeout, для failure detection

Зачем нам сравнивать мгновенные показания времени? Для того, чтобы упорядочивать собычтия
У нас есть Петя, и он сделал запись X, и получил подтверждение. Затем Петя сказал Васе про изменение. А Вася, вместо того, чтобы читать, сделал новую запись Y. Запись X случилась до записи Y, и мы ожидаем, что если после всего этого пойти в систему и спросить "Что лежит по ключу K?", то она вернет Y. Для этого система должна понять, что в нее записали X и записали Y, и при этом запись Y - это более свежая запись. Как бы мы могли это достичь? Пусть узел, который получил запись X, присвоил ей временную метку - показание локальных часов: 12:00. А после записи Y другая машина присвоила записи временную метку 11:59. Потому что часы разное время показывали. Получилось, что для системы X произошел раньше, чем Y

**Scew** - рассинхронизированные часы

**Drift** - часы могут тикать иногда быстрее, иногда медленнее, чем идеальные часы

![alt text](images/5.png)

## Синхронизация часов, нижняя оценка
Представим очень простой мир, где есть два узла. И узел n1 хочет свести свои часы с узлом n2
![alt text](images/6.png)
Пусть у нас есть ограничение на мир: любое сообщение доставляется в таком промежутке времени: $delay(m) \in [\delta-u, \delta]$
, где u - uncertainty , в то время как drift никакого нет
Синхронизировать часы (в такой модели) - это значит узлы должны выбрать своим часам какую-то поправку: $sc_i(t) = c_i(t) + o_i$
Кто-то в таком мире принес нам алгоритм и говорит: "Его можно запустить на двух узлах и через какое-то время можно подобрать такие поправки, что: $|sc_i(t) - sc_j(t)| \le \varepsilon$ "
Насколько маленьким может быть этот эпсилон? Рассуждения демонстрируют общую технику, которая будет использовать дальше:
Мы построим два исполнения, и они будут разными, которые, во-первых, будут различными, т.е. будут давать разные поправки в двух исполнениях, а во-вторых, алгоритм на двух узлах не смогут эти два исполнения отличить друг от друга, поэтому они выберут одинаковые поправки. И на этой коллизии у нас построится нижняя оценка

Мы управляем сетью и у нас есть два узла: $n_1$ и $n_2$. Пусть у нас от узла $n_1$ к узлу $n_2$ сообщения идут максимально долго насколько могут, а в обратную сторону максимально быстро. Мы строим такую сеть и алгоритм работает и синхронизирует часы. А теперь мы сделаем операцию, которая называется сдвигом - **shift**. Мы все события, которые происходили на узле $n_1$, оставляем на тех же местах, но переворачиваем сеть: т.е. теперь сообщение от узла $n_1$ к узлу $n_2$ летят очень быстро, а обратно очень медленно. Что произошло? Для узла $n_1$ ничего не произошло вообще, он не чувствует разницы. А для узла $n_2$ событие съехало немного назад, но он же не тупой и может это понять: в первом случае он получил в 12:00, а во втором случае в 11:59. Во втором исполнении переведем часы второго узла на $+u$. Утверждается, что хоть и во времени все съехало, но у $n_2$ нет возможность это почувствовать, потому что времени он не знает, он знает только то, что показывают ему часы, а часы показывают ему то же самое. Что мы получили? Мы запустили алгоритм в первом и втором исполнении, он синхронизировал часы. И в обоих случаях часы, синхронизированные для $n_1$ вообще не отличаются. Посмотрим на $sc_1(t*)$. Они принадлежат от -eps до +eps. А теперь подумаем, как работают синхронизированные часы для $n_2$. Алгоритм не чувствует разницы на узле $n_2$ между двумя исполнениями, а значит он выберет одну и ту же аддитивную поправку, но как бы сами часы отличались на $u$, т.е. синхронизированные часы узла $n_2$ в этих двух исполнениях должны отличаться ровно на $u$, и при этом они оба должны отличаться не более, чем на eps, чем $sc_1(t*)$. Т.е. $\varepsilon$ $\ge u/2$
![alt text](images/7.png)
В случае n узлов оценка немного меняется: $\varepsilon$ $\ge (1 - 1/n)u$
## GPS и синхронизация часов
У нас беда с часами - мы не можем оценить асимметрию в сети, мы не можем оценить время round trip'а, а может быть иногда у нас нет этого round trip'a, у нас коммуникация односторонняя, и время доставки сообщения мы знаем довольно таки неплохо

Как найти нас на плоскости
![alt text](images/8.png)

Добавим реализма: мы вышли в лодке в туман рыбачить. У нас есть карта и часы. На карте отмечено 3 маяка. Мы знаем, где они находятся, но не знаем, где мы находимся. Но мы знаем, что маяки раз в 5 минут издают какую-то пронзительную и уникальную сирену. И тогда наша система становится вот такой уже:
![alt text](images/9.png) - навигационное уравнение GPS
4 координаты, 4 уравнения, 4 спутника
Система GPS нас позиционирует не только в пространстве, она нас позиционирует по времени, и чтобы позиционировать в пространстве, ей нужно синхронизировать часы на ресивере с часами в космическом сегменте спутника

## Google Cloud Spanner, Google TrueTime, ожидание вместо коммуникации
Непонятно, откуда тут возник GPS
Почти все те системы, про которые было написано выше, впервые написали в гугле.
Spanner - это одна из систем, которая предоставляет гугл. Это геораспределенная база данных, и чтобы ее написать, инженерам гугл понадобился новый подход к часам. Они используют часы в своих алгоритмах, но не now(), потому что доверия к ней нет. Они построили сервис - [TrueTime](https://cloud.google.com/spanner/docs/true-time-external-consistency). Как все это устроено? Мы можем на каждой машине спросить:

    TT.Now() -> [e, l]
    // e - earliest
    // l - latest
И система TrueTime гарантирует, что настоящее время внутри этого интервала
Чуть формальнее: если мы задаем запрос TrueTime.Now() в момент времени $t_0$, и получаем ответ в момент времени $t_1$, то этот интервал $[t_0, t_1]$ пересекается с интервалом $[e, l]$. Беда в том, что мы не знаем $t_0$ и $t_1$
Нам гарантируют это, и стремятся сделать так, чтобы ширина этого интервала была как можно меньше: $|l - e| \le 6ms$

Как это связано с GPS? TrueTime работает на каждой машине. И в датацентре помимо обычных машин, которые пользуются этим сервисом, стоят т.н. time-master'а. Бывает два типа тайм мастеров:
1) Машины, на которых установлены GPS антенны, которые синхронизируются с GPS для того, чтобы синхронизировать часы
2) Armageddon master - машины с атомными часами. У нас атомные часы в космосе, атомные часы у нас (для отказоустойчивости)
Исходя из всей этой информации, каждая машина выводит себе оценку этого интервала $[e, l]$. И выводит забавно: раз в 30 секунд каждая машина общается с этими мастерами и получает себе оценку интервала. А теперь через 25 секунд приходит Google Spanner и спрашивает у TrueTime текущее время. Что вернет нам TrueTime? TrueTime закладывает между точками синхронизации дрейф в 200 миллионных долей и вернется промежуток больше

Читать как иврит справа налево
![alt text](images/10.png)

## Зачем распределенным системам TrueTime?
Время - это такая хитрость, общее разделяемое состояние между машинами в сети. Чтобы мы могли что-то сделать, надо чтобы машины поговорили друг с другом, чтобы они друг о друге что-то узнали. У них есть что-то общее - время. Они могут говорить явно друг с другом, отправляя сообщения. Google используют TrueTime для того, чтобы не говорить. Они строят не просто базу данных в кластере, они строят геораспределенную систему такую, в которой узлы стоят физически далеко друг от друга. И когда мы общаемся по сети внутри ДЦ, там действительно могут быть тайминги очень маленькие (1ms). Когда мы пересылаем байтики через Атлантику или с Восточного на Западное побережье США, там уже тайминги сотни миллисекунд. И мы с помощью некоторой хитрости можем понять, как можно эту коммуникацию, которая может быть долгой, можно заменить на локальное ожидание, т.е. мы просто сидим и ничего не делаем, и засчет этого что-то становится лучше. Но сидим мало: 6ms, вместо того, чтобы общаться долго и ждать 100ms. Это большая хитрость за счет того, что время у узлов общее.

## Итоги
Можно сказать следующее: (в данном курсе) мы живем чаще всего в асинхронной модели, т.е. мы не делаем предположений о скорости доставки сообщений, мы не делаем предполоожение о скорости работы часов, и мы не делаем предположений о дрейфе часов (в тех случаях, когда мы доказываем, что системы не нарушают каких-то гарантий, т.е. safety. Когда мы доказываем, что система не делает ничего плохого, мы используем асинхронную модель, где нет параметров времени никаких. Когда мы доказываем, что система вообще говоря делает что-то полезно и отвечает пользователю, то мы тогда привлекаем таймауты, какие-то оценки, время доставки сообщений и прочее)



# Семинар 1. Среда исполнения распределённой системы
*Когда у нас есть одни часы, мы знаем, который час, но когда у нас много часов, мы плохо понимаем, что происходит*

## Разные временные оси
[Иллюстрация временных осей в мире](http://leapsecond.com/java/gpsclock.htm)

![alt text](images/12.png)

* TAI - международное атомное время (названо по-французски, поэтому буквы в непривычном).
Почему атомные часы, а не что-то другое? Тут нужно понять, что мы вообще понимаем под временем, что мы считаем эталонным временем? 
Раньше люди использовали естественную меру времени: астрономическую. Например, когда солнце в зените, а затем когда оно снова в зените, то видимо прошел день. Чтобы из этого получить секунду, мы делим сутки на 86400. Мы получили секунду, но в чем беда? Земля несколько неравномерно вращается, т.е. Земля - это не самый точный периодический процесс. Если мы хотим определить секунду через какое-то астрономическое явление, связанное с Солнцем и Землей, то нам нужно что-то усреднить еще. Когда-то секунда равнялась 1/86400 какого-то среднего дня. Сейчас секунда определяется иначе: [Секунда — время, равное 9 192 631 770 периодам излучения, соответствующего переходу между двумя сверхтонкими уровнями основного состояния атома цезия-133.](https://ru.wikipedia.org/wiki/Секунда). Теперь астрономические явления становятся отвязанными от определения секунды.
* GPS - там тоже атомные часы, но время отличается. На самом деле атомные часы в космосе и атомные часы на Земле - это немного разные часы ([пруф](http://themarginal.com/emc2/applications_of_relativity_in_gps.htm)). В DC серверы стоят в стойках, их кто-то охлаждает, температура в стойке вверху и внизу разная, кварцевые часы очень чувствительны к температуре и они дрейфуют по-разному наверху и внизу стойки, уже расходятся и очень неприятно. Атомные часы вообще супер чувствительные: если поставить их напол, а потом поднять их на руки, то они уже по-разному будут идти. В случае GPS мы их вообще в космос запускаем: там гравитация меньше, и часы летают вокруг земли на огромной скорости. ОТО предсказывает, что атомные часы, находящиеся на орбите Земли, будут тикать чуть быстрее, и будут опережать атомные часы на земле на 45 microseconds per day. В то время как СТО предсказывает, что атомные часы, летая на большой скорости, будут двигаться немного медленнее, на 7 microseconds per day. И когда их туда запускают, их заводят так, чтобы они на 38 микросекунд медленнее шли, чем на Земле, чтобы с ними можно было сопоставляться на Земле. **Но это все забавная история, дело на самом деле не в этом. Все из-за того, что есть UTC.**
* UTC - есть два способа это расшифровать. С одной стороны, это Coordinated Universal Time (англ.), с другой стороны это Temps Universel Coordonne (фр.), и по какой-то причине ни одно из них с UTC не складывается. Беда в том, что довольно плохо, что мы отвязали секунду от астрономических явлений, но при этом мы их как бы наблюдаем, и давайте сделаем так, чтобы сходилось одно с другим. Если Земля вращается немного быстрее или медленнее относительно эталона секунды, то мы сделаем так, чтобы если вдруг у нас накапливалась лишняя секунда или куда-то пропадала из вращения Земли, то мы бы ее либо добавляли, либо извлекали из этого дня. Так получается явление под названием **секунда координации (leap second)**

## Високосные секунды
[Статья на вики](https://en/wikipedia.org/wiki/Leap_second)

Земля вращается то чуть медленнее, то чуть быстрее, поэтому иногда в день вставляют лишнюю секунду (которая выглядит как 23:59:60). Это такое техническое решение, чтобы связать временную ось и Солнце. Именно поэтому у нас есть три шкалы: UTC, GPS, TAI.

* TAI: никак не учитывает секунды координации, оно считает секунды по определению секунды, т.е. нет високосных секунд.
* GPS: тоже считает секунды по определению секунды, но эта ось времени запустилась немного позже, и оно было сначала синхронизировано с UTC, в итоге в GPS есть несколько високосных секунд.
* UTC: есть все високосные секунды

Зачем же нам об этом знать (кроме того, что это забавно)? Мы сделали астрономов счастливыми, а пострадали разработчиками, поскольку работа со временем это коллосальная боль всегда (например, time zones). Есть проблема: например, есть [Unix time](https://en.wikipedia.org/wiki/Unix_time), где в сутках 86400 секунд, но в сутках бывает +- 1 секунда еще, и этот Unix time ведет себя странно: он возвращает количество секунд с начала эпохи, но при этом иногда нужно в эти тайм стемпы добавить лишние секунды, какой-то тайм стемп повторяется. Т.е. наш Unix time ведет себя иногда немонотонно, а ведь это "очень естественно" ожидание от времени, что время идет назад xD. Короче, мы можем пострадать в своих распределенных системах, если будем полагать на монотонность течения времени.

[Пример, когда так пострадали](https://blog.cloudflare.com/how-and-why-the-leap-second-affected-cloudflare-dns/). Здесь часы в Go шли немонотонно, после чего родился такой [issue](https://github.com/golang/go/issues/12914), что хорошо бы, когда мы вычитаем две временные метки, мы всегда получали бы что-то неотрицательное. И если мы сейчас посмотрим на [реализацию часов в Go](https://github.com/golang/go/blob/master/src/time/time.go), то там есть на самом деле два числа:

    wall uint64
    ext int64
И что это значит? На самом деле у нас в компьютере два вида часов, которыми мы можем пользоваться. Т.е. два типа часов, не в смысле какого-то физического механизма, а гарантии и операций, которые над ними допустимы. 

## Монотонные часы
У нас есть Wall time часы - они показывают время, которое мы ожидаем, это время на разных часах в разных узлах у этих часов общая точка отсчета, но они видут себя немонотонно. Второй вид часов - монотонные часы, они ведут себя монотонно, но у них нет точки отсчета общей.

Мы можем сравнивать показания wall time часов с разных машин в нашем алгоритме, но мы не можем рассчитывать, что эти часы синхронизированы вообще, и мы доказали, что этого не добиться. Но вообще сравнение показаний часов на разных машинах - это физически разумная операция. А вот сравнение показания монотонных часов на разных машинах - это абсолютно бессмысленная операция, потому что у них разная точка отсчета, но мы можем заводить какие-то timeout, timer и прочее.

Что же случилось в Go? Добавили в реализацию часов две компоненты: wall time и показания монотонных часов. Например, если мы замеряем интервал времени, то нам не нужнен wall time.

[То же самое есть в C++](https://en.cppreference.com/w/cpp/chrono#Clocks): [system_clock](https://en.cppreference.com/w/cpp/chrono/system_clock) и [steady_clock](https://en.cppreference.com/w/cpp/chrono/steady_clock). Т.е. когда мы вычитаем показание now, мы всегда получаем какой-то duration некоторый, но вот операции, которые допустимы с самими временными метками, это уже разные наборы операций для этих типов часов.

Теперь представим, что мы пишем не одну программу, которая использует время, и должны быть аккуратными, а мы пишем целый Google, и у нас очень много программ: у нас очень много сетевых приложений, тысячи разработчиков, которые написали многие миллионы строк кода, и нам говорят: "Скоро в Time scale (во время) вставят високосную секунду", и мы в панике, потому что кто знает, где эта секунда взорвется, что взорвется от того, что у нас часы пойдут немонотонно в какой-то момент. На самом деле это давно решенная проблема, и способ ее решения называется [Leap Smearing](https://developers.google.com/time/smear). Это решение не на уровне приложений, конечно. Мы не можем заставить всех разработчиков проверить свой код и нигде не ошибиться. Вместо этого мы можем просто избежать этот ход назад: мы можем в своем DC развернуть собственный сервис NTP, который будет подсказывать компьютерам в этом DC несовсем честные секунды, а немного увеличенные секунды. Секунды у нас будут идти чуть медленнее, чем должны, но зато их будет правильное количество, и тогда у нас time stamp'ы не пропадут и не повторятся.

## Устройство датацентра
Мы хотем делать отказоустойчивые системы, и эта отказоустойчивость достигается не только на уровне алгоритмов, но и на уровне железной инфраструктуры. Например, Google пытается всё резервировать, в том числе и источники питания (вдруг сбой), и кабели (кто-то порвет сети: экскаватор, грызуны и пр.).


[Статья Google](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf), в которой они пишут, что их сеть глобальная, но при этом частная: все байты, которые путешествуют внутри систем Google, они путешествуют только по их каналам, по их коммутаторам, по их оборудованиям. И они его контролируют и резервируют на всех уровнях, в т.ч. прокладывают собственные кабели по морскому дну.

Что происходит внутри DC? Есть какие-то шкафы - **Rack** (стойка). Все машины, которые есть в DC, уложены в стойки. В [DC Яндекса](https://yandex.ru/company/technologies/datacenter) стойки содержат до 80 серверов, и они подключены к внешнему миру и другим серверным стойкам через коммутаторы, которые в этих стойках стоят. При этом, если в стойке выходит из строя коммутатор, то мы теряем не одну машину, а сразу несколько.

**Failure domain** - часть системы, которое выходит из строя, в результате какого-то одного сбоя.

Если мы хотим хранить что-нибудь надежно, то мы должны хранить это на несколько машинах. При этом если реплики нашей системы, которые хранят одни и те же данные, находятся в одной стойке, то поломка сетевого коммутатора приводит к тому, что мы теряем все копии. В реальных системах нужно это учитывать (пример: система хранения HDFS (там это учитывается)).
Мы должны резервировать коммутаторы, причем даже мб от разных производителей.

Серверные стояки мы должны объединить в один кластер. [Доклад Google про все на свете](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf) - на заре времен делали так:
![alt text](images/13.png)

На заре времен делали так: брали эти стойки, у каждой из них был коммутатор, который называется **top of rack switch** (потому что он наверху стойки, но на самом деле он может быть не наверху), и у нас есть такой агрегирующий **switch**, который соединяется со всеми стойками, и дальше все через этот switch могут общаться друг с другом. Но есть проблема: если у нас это оборудование ломается, то весь наш кластер разваливается, у нас единая точка отказа (**single pointer failure** - вывод какого-то одного компонента не должен приводить к тому, что вся система становится недоступна). Другая проблема: эта коробка не резиновая, т.е. у нас ограниченное количество портов. Третья проблема: как жили большие компании в 2010-х? У каждого свои сервисы, свои машины для обработки запросов, каждому сервису нужны физические машины, чтобы запускать код. Любой сервис должен рассчитывать на пиковую нагрузку, а для этого нужно ресурсы резервировать, т.е. мы заказываем больше машин, чем обычно используется, а потом машины просто стоят и не используются, что является большой тратой в больших масштабах. К чему пришло человечество?

## Устройство сети
Не нужно делать много маленьких кластеров, нужно делать один большой гигантский кластер. У нас система геораспределенная, и мы хотим в каждой точке сделать очень большой кластер, где будет десятки тысяч машин, и что мы хотим от такой сети? Во-первых, мы хотим, чтобы в такой сети не было много избыточности, в смысле маршрутов, чтобы ни одна какая-то сетевая коробка не приводила к тому, что какие-то две машины потеряют связанность. Во-вторых, мы хотим, чтобы у нас была очень большая пропускная способность любого разреза в сети. В-третьих, мы хотим, чтобы в нашу сеть можно было спокойно добавлять новые машины, чтобы она масштабировалась. В-четвертых, мы хотим, чтобы в ней не требовались специальные какие-то коммутаторы с очень большим числом портов.

Как собрать такую сеть? [Старое решение](https://engineering.fb.com/2014/11/14/production-engineering/introducing-data-center-fabric-the-next-generation-facebook-data-center-network/) и [новое решение](https://engineering.fb.com/2019/03/14/data-center-engineering/f16-minipack/) из facebook. Посмотрим на старое решение, ибо в новых топологиях что-то очень громоздкое:
![alt text](images/14.png)

Серые точки - это стойки. Сам кружочек - это коммутатор, который их связывает с внешним миром. Из этих стоек мы собираем т.н. поды (**pod** - единица расширения сети, т.е. если нам нужно добавить новые машины в сеть, мы строим новый pod):
![alt text](images/15.png)

Мы хотим, чтобы внутри этого пода у нас была какая-то избыточность. У нас есть 4 switch'a, и мы к каждому из них цепляем все серверные стойки. В итоге у нас есть 4 способа каждой машине с каждой поговорить.

А дальше мы делаем много таких подов, и нам теперь нужно, чтобы мы могли связать стойки из разных плоскостей. Для этого мы пересекаем их перпендикулярными плоскостями (Spine Planes), которые организованы точно также. В основании этих плоскостей вот эти самые свитчи из подов. Spine Switches объединяют Fabric Switches.

Итого у нас изобилие маршрутов, которыми мы можем связать две любые машины в этом кластере. Реальная сеть выглядит так:
![alt text](images/16.png)
все намного сложнее, и ломается это все тоже интереснее.

Чем отличается ожидание от реальности (в смысле времени доставки сообщения). Ожидание: берем линейку, прикладываем к глобусу, получаем расстояние, и, учитывая скорость света, получаем время. Реальность: оптоволокно - это не вакуум, там есть некоторый коэф. преломления, так что скорость примерно в 1.5 раза ниже, по пути еще масса разных неприятностей, например, очень сложно положить непрерывный кабель, потому что он катушками продается, и нужно их спаивать, и в этих местах появляются какие-то погрешности, нужно усиливать сигнал, и это все замедляется (или то, что экскаваторы рвут кабели, и нужно что-то заново сваривать). Что еще может влиять на задержку сообщения между двумя DC? Нам нужно несколько проводов: например, один раскопают, другой на техобслуживании, и нам нужен третий, который нас спасет, и глупо их тянуть рядом, потому что не сработает, в итоге мы тянем какой-то провод по какому-то затейлевому маршруту, и длина увеличивается. Кроме того, очень странно соединять DC просто друг с другом, нужно соединять их с людьми: если окажется, что у нас есть два DC, а между ними Москва, то мб нужно тянуть провод не от двух DC друг то друга и до Москвы, а нужно просто Москву соединить с двумя DC. Дальше еще хуже: если мы хотим соединить две коробки DC по прямой, то это очень сложно сделать в реальности, из-за того, что кучу всего нужно согласовывать с кучей владельцев чего-либо. В итоге задержки увеличиваются из-за таких ограничений нашей реальности.

Но мы же сказали, что мы ничего этого не наблюдаем, т.к. у нас есть протокол TCP, который от нас все это скрывает.


## TCP
Полезно понимать, как устроен TCP, т.к. наша абстракция надежного канала более-менее повторяет идею TCP, вдобавок TCP сама по себе распределенная система.

На уровне сети нет никакого протокола TCP. Сетевые коробки между нашими машинами ничего не знают про наш протокол. В итоге TCP, вот эта абстракция соединения, существует только в головах двух машин, в операционных системах двух машин, и физически этот провод никак не представлен, никто о нем больше не знает. **[И это приводит к довольно странным ситуациям](https://web.archive.org/web/20220823105029/https://tritondatacenter.com/blog/tcp-puzzlers)**. У нас есть сервер kodos, клиент соединяется с ним и называется kang, и мы перехватываем все системные вызовы. Как же TCP соединение будет вести себя, если мы будем как-то воздействовать на сервер и клиент. По-началу они соединились, и мы видим на сервере и на клиенте TCP соединение. 

Сначала убьем серверный процесс: машина жива, а процесс погиб, а клиент нам что-то отправил, и хочет что-то получить обратно. Что будет с его взглядом на соединение с его стороны? TCP реализован ОС; ОС видит, что процесс умер, и клиенту можно отправить сообщение о том, что больше ждать нечего, и клиент узнает, что поток с его стороны закончился.

*[Есть замечательная книжка](https://hpbn.co/building-blocks-of-tcp/#congestion-avoidance-and-control) про то, как делить общую сеть, не зная про других клиентов, как адаптировать свою нагрузку на нее*

А что будет, если мы перезагрузим сервер? Вообще говоря, клиент ничего об этом не узнает, потому что у него в голове соединение есть, он, например, ничего в сеть не пишет, а сервер забыл уже про все, потому что он перезагрузился и все с чистого листа. Т.е. если мы перезагрузим сервер, а затем на клиенте напишем write в socket, то вот этот сам системный вызов завершится успешно. И когда мы записали сообщение серверу, а потом ждем ответа, то в обратную сторону мы уже получим ошибку, потому что сервер уже не знает про нас, уходи, и мы уходим. 

А что, если мы вообще выключим сервер? Попробуем что-то записать, и что с ней будет? TCP - это транспортный протокол, он реализован протоколом маршрутизации над IP, и потеря какого-то пакетика нормальная ситуация, и сам TCP это учитывает. TCP предоставляет нам абстракцию потока байт, и мы если мы не можем отправить весь поток, то мы отправляем такие кусочки этого пакета, и если нам сервер говорит, что он получил там 4,5,6 сегмент и 3 не получал, то мы его переотправляем, т.е. если что-то потерялось то это мало что означает, и TCP не будет воспринимать это как проблему. Это, конечно, некоторая трудность, но мб она разрешится, мб сеть сейчас перегружена. Мы не можем на стороне клиента понять две конфигурации мира: где с сетью проблемы, и где узел. Но у нас есть какой-нибудь механизм keep_alive и рано или поздно он сработает, допустим по какому-то очень большому таймеру, и мы все же соединение прервем. Это произойдет очень нескоро, и мы должны писать код, который учитывает это, что мы не должны столько ждать, что машина уже мертвая, но при этом соединение открыто еще очень долго.

TCP подвержена тем же самым проблемам задачи синхронизации часов, что и сами распределенные системы: мы не можем изнутри одной машины понять, что происходит глобально в мире, поэтому наши знания всегда неточные и неполные, и поэтому когда мы пишем код, мы должны хорошо понимать, к каким конфигурациям это может привести.


## Дополнение про всякое
В системе Google Spanner используется другой подход: мы, как инженеры гугл, не считаем, что все ломается, а наоборот: мы решаем задачи отказоустойчивости не только на уровне алгоритмов, мы опускаемся еще ниже и начинаем решать задачи на уровне самой железной инфраструктуры. И мы не строим надежные алгоритмы над неустойчивыми уязвимыми физическими абстракции, мы наоборот делаем физический мир устойчивым: мы делаем надежные часы, которые не могут бесконечно дрейфовать и на которые мы можем рассчитывать, мы строим глобальную сеть, в которой не бывает partition'нов, потому что все зарезервировано, все на свете. И в таких предположениях нам немного легче писать код. Но это работает только если мы работаем в гугл, потому что никто такой подход повторить не может.

Еще один важный момент, который нигде в учебниках не отражен, и о нем только начинают писать в статьях, состоит в том, что распределенные системы, которые пишут люди, в основном работают не прямо в DC, а они работают в облаках, и тут появляется еще один слой абстракции и еще один слой косвенности - это виртуализация, это контейнеры, это планировщики кластеров, которые выделяют виртуальные машины, которые делят между ними ресурсы. Современные системы должны учитывать даже это, если хотят быть производительными.

Если знания и существуют, то только в статьях, а ни в каких учебниках, потому что они отстают и их пишут очень странные люди, которые занимаются очень абстрактными задачами, а мы хотим говорить про те задачи, которые мотивированы реальной жизнью, которые где-то у кого-то возникают. И только из статей этих людей мы можем узнать, как все происходит, из статей, как они что-то сделали и чему научились.


# Лекция 2. Линеаризуемость. Репликация регистра, алгоритм ABD
## KV Storage
### История
Мы построили модель, которая будет решать наши задачи. Пока что мы живем в моделе, где узлы отказывают, просто выключаясь навсегда. Т.е. узлы ведут себя в планах протокола, и злоумышленников у нас нет

KV Storage
* Set(key, value)
* Get(key)

В начале 2000-х годов Google и Amazon написали статьи про Google Bigtable и Amazon Dynamo, это были KV системы
### Слои архитектуры распределенной БД
Почему мы говорим о том, что мы все еще ничего не умеем? Если мы посмотрим на современные БД, под капотом они реализованы поверх KV Storage

Мы хотим сделать KV Storage. Что это значит? Оно должно масштабироваться и быть отказоустойчивым. Но задачу мы начинаем решать на уровне одной машины. В чем сложность? В том, что машина должна переживать рестарты, т.е. хранить не в памяти, а на жестком диске. У нас API с произвольным доступом - у нас есть произвольные ключи и мы что-то по ним спрашивам: пишем или читаем. А диск так не умеет - он умеет читать и писать только последовательно
Поэтому первая задача, которая возникает в пределах одной машины - это реализация хранилища. Мы должны на самом низком уровне в пределах одного узла построить эффективно систему с произвольном доступом поверх диска, который умеет последовательный доступ эффективно (это системы типа LevelDB и RoseDB)
Теперь мы хотим добавить отказоустойчивость: т.е. мы начинаем данные реплецировать, чтобы переживать смерть отдельных дисков
Далее у нас данные перестают вмещаться на одной машине, и мы начинаем заниматься распределением этих данных по кластеру: у нас есть теперь много машин и очень большой диапазон ключей, каждый из них мы назовем range - ключи $[b, d]$. Мы скажем, что каждый range будет реплецироваться независимо. Т.е. для каждого диапазона будет несколько машин, которые реплецируют этот диапазон. Скажем, что на этом уровне мы наши данные шардировали
![alt text](images/11.png)
Наконец, последний уровень - уровень транзакций, потому что без транзакций в KV Storage мы не получим транзакции на уровне SQL
// TODO, остановился на 10.30